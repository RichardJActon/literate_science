# Units of publication

## The new hierarchy of publication types

The traditional hierarchy of scientific publications looks something like this:

-   Textbooks
-   Reviews
-   Primary papers

Running from the narrowest to the broadest scope and most detail to the highest level of synthesis and abstraction, in theory. 
This is an oversimplification and we have been adding to the ontology of publication types somewhat in recent years, more on this later. 
Some additions such as pre-registrations have caught on more in some field that others.
Whilst some innovations are reasonably domain specific others could benefit from being more widely adopted.

In addition to the mainline scientific communication hierarchy there is of course the communication of science to a broader audience.
Ranging from cross disciplinary communication to communicating with the public, policy makers, and in education. 
These essential functions of the scientific community with immense civic import that are undervalued in the current model.

Integrating many of the new forms of publication that have arisen I would propose a new more granular hierarchy:

-   Science communication (multimedia, curricula, policy briefs etc.)
-   "git-books" - much like a textbook but under continuous revision
-   Topic Reviews, Curated Best practice resources for lab protocols and computational pipelines as well as, Benchmarking resources
-   Theory, synthesis and prediction (without a requirement for experimental detail but ideally with resolution criteria)
-   Experimental results
-   Data, Methods, Protocols, Pipelines, & Software publications
-   pre-registrations / experimental plans/designs

We have made some progress towards this model already, everything that I list here already exists in some form after all.
We have however not really thought or talked about them much when put together as a whole - at least not in my experience.
Bringing this more granular view of the unit scientific publication to the fore I think has a number of advantages.

> Consider implications of new forms of publication for existing ones

## The minimum unit of publication

Let us step back for a moment a consider what is the minimum unit of publication? According to the proponents of 'nanopublications' this minimal unit of publication is a simple statement consisting of a subject, object predicate triple that is generated by some author and asserted by some party [@groth2010].

> *e.g.* the journal of the widely understood asserts that the statement "Malaria is transmitted by mosquitoes" was authored by John Smith.

This minimal unit of publication structured in this fashion has a number of very useful properties for automating the creation of a web of semantic meaning in the scientific literature.
Such structure has considerable potential to increase the ease and effectiveness of literature mining and may permit semi-automated means of assessing the weight of evidence supporting a particular statement and identifying gaps and weaknesses it the current state of understanding of particular topics.

semantic web rdp graph structure etc. https://en.wikipedia.org/wiki/Semantic_Web

current nano-publication infrastructure need and of work before it will be a smooth easy to use experience for most would be authors and I suspect also some major performance optimizations for handling large datasets.

However, it would be somewhat cumbersome and impractical to write only in nano-publications, whilst they may be convenient for a machine legibility they are less convenient for human legibility and I envisage them catching on once at least semi-automated tools can convert a more human unit of publication into a collection of nano-publications.


Modern publications particularly in the life sciences (my home territory) are too long, too complex, too narrative driven and, in partial contradiction with being too long are often too compressed.
I'll be expanding on these points, but for now take my word for it that this is not merely me complaining about trying to read 50 page cell papers and still have the time to do anything else.
As the ability to do experimental work has increased in the life sciences due to technological advances that have rendered some projects that would have taken decades 20-30 years ago achievable in months the expectations for publications have changed.
Reading papers from the 1980's and 90's one is struck by their relative shortness and simplicity compared to many modern papers.
Indeed, empirically scientific manuscripts are getting [harder to read](https://doi.org/10.7554/eLife.27725.001) [@plav√©n-sigray2017].

*Check the data on length, has it actually increased?*

I would argue that is this mostly not due to an intrinsic increase in the complexity of the systems we are able to study and the methods that we are using to study them. 
These have increased and some of the added difficulty may be accounted for by the increasing number and narrowness of specialties but I think more blame is due to failure to adapt institutional structures around ever more and narrower specialties as well as publication practices.

Annecdotally it is my impression that the number of experiments and the technical complexity of those experiments per paper has been on the rise *(I would welcome empirical evidence from literature mining on this subject to confirm or disconfirm my conjecture here).*
It is marvelous that we can get more done and that we have sophisticated new tools with which to work, however this presents a number of problems when publishing. 

Firstly for the peer review process: For an increasing number of papers the scope of what they investigate exceeds the reasonable ability of the typical number of reviewers to adequately assess the material.
Domain expertise in the system being studied, statistical expertise, methodological expertise in the methods employed are all needed for a rigorous assessment of most papers. 
The larger and more complex the unit of publication the greater the opportunity for [diffusion of responsibility](https://en.wikipedia.org/wiki/Diffusion_of_responsibility) among the reviewers who may be free to assume that one of the others will give adequate scrutiny to aspects outside their direct expertise, this may not be the case. 
As papers cover an increasing number, and more complex experiments the probability of error in some part of the work increases, as the more things are in a unit of publication the more chances there are for at least one of them to be in error.
I suspect that error increases at a rate greater than that which you would expect from simple conjunction, due in part to the diffusion of responsibility for review earlier hypothesized earlier. 
I would contend that the increasing complexity of papers contributes to the replication problems commonplace in many field in part because of the challenges of adequately reviewing them.

I propose experimental tests of the diffusion of responsibility in reviewers and effectiveness of reduced publication size/scope at reducing it.
For example one could send a manuscript with a statistical error to two types of sets of reviewers. 
one pool of reviewers none of whom have stats expertise and count how often to they a. catch the error, b. flag their lack of expertise and ask for review by a stats expert c. take no action. The second set of reviewers should contains a stats expert to establish a base rate for missing the error.
Try this for a conventional publication and 'micropublication' containing essentially only the experiment with the error.

That addresses criticisms of too long and complex, now for the related problem of being what I think of as too compressed. 
The text of papers in nature for instance is almost always to short to adaquately convey the necessary detail for the amount of work that has been done to justify a place in such a prestigious publication. It is a primary publication so must be precise and near exhaustive in its description of the work done

Excessive narrative seeking can contribute to the file-draw effect, if you can't make a good story of it you might not publish it. The expectation of turning work into a 'full paper' can also have bad incentives, for some project you go out on a lim and it does not work in way that left you write a full paper but data generated and other aspects of the work are

This greater specialization of publication types reduces citation dilution, increases reviewability by narrowing the scope of expertise necessary to evaluate smaller publications. It also pushes the making of narratives away from the primary research where it can contribute to the file draw problem and pushes it to it's own 'sesense making' layer. This also reduces time to publication it can take years to get things published under the current paradigm, this should tell us publications are too big and too complex and need to be broken into smaller parts to shorten the feedback loop of the review process.

This stack also enables a new form of professional scientific specialization not purely in the dimension of subject specialty but at level of analysis, you could specialize in sharpening up the theoretical models or experimental design in a broarder array of diciplins publishing mostly at the theory level or generate a lot of high quality data. High level synthesis, curation and communication of important scientific discovery in the field as gitbook maintainers to keep the 'textbooks' current on an ongoing basis without the constraint of editions whilst remaining citable because of version control. Increases accessibility to publishing to students and non-domain experts making smaller contributions that are still in the public interest to have published. It may be argued this this approach increases the amount of 'noise' there are already so many publications that it is impossible to keep up in many fields, this is part of why new specialties are needed in the synthesis of experimental information into models. It is only possible for individuals to keep up will all the primary publications in a few very narrow specialties...

Another element that this model contributes in greater codification of understanding that often goes un codified both at the level of specific methodological detail and at the level of the mental models that researchers are using ot reason about their research.

-   currently lacking a self regulatory mechanism to govern the balance between publications of the different types so we don't end up with a bunch of unanalysed data / too much theorizing?

Don't get me wrong there are a lot of good papers out there but increasingly this seems to be in spite of and not because of the publication medium.

smaller papers = fewer co-authors, thus reduced collective action problem of choice to publish in alternate publications if you don't know your co authors that well you are more likely to assume that they will want to publish in conventional high impact journals and not be too keen on you deciding to publish in a less fashionable venue so we default to the conservative option of a conventional option rather than risk getting into an extended email argument about the benefits of open science with collaborators who we want to keep happy.
We are [stuck in a bad nash equilibrium](https://equilibriabook.com/molochs-toolbox/) because of our assumptions about our co-authors and they may not be as valid as we conservatively expect.


better documentation of informal replications performed as precursosrs to other work
