[["intro.html", "Literate Science Section 1 Introduction 1.1 Literate Science", " Literate Science Richard J. Acton 2021-09-05 Section 1 Introduction 1.1 Literate Science A new paradigm for academic publishing This is currently very much a work in progress and may change dramatically and rapidly, some sections may be drafts, outlines and bullet points to be filled in and properly edited as I go This is an outline for a new academic publishing paradigm with reforms spanning the types of article we publish, the formats in which we author them and the business models of publishers. These proposals aim to address a number of the systemic issues by which academic publishing is currently beset. They range from the seemingly minor impracticalities associated with submitting papers through to the extractive business practices of a number of large incumbent publishers, the vicissitudes of peer-review, irreproducible analyses, the form and function of papers and a number of other issues. These proposals draw on ideas from previous ‘unconventional’ publication platforms such as f-1000, peerj, elife (particularly their libero publishing suite), JoVE - journal of visualised experiments, and microPublication. They also draw from RopenSci’s software review process and Journal of open source software JOSS, indeed extensive inspiration is drawn from the free &amp; open source software communities more generally. In addition they draw from the ideas of nano-publications and single observation publishing from sciencematters.io, as well as community contributions to shared protocols and pipelines from protocols.io and nf-core. Furthermore a number of tools, ideas and practices from modern software development contribute to the bases of some of the proposals to follow. These include: The concept of literate programming in which code and it’s documentation, context and explanation are part of the same corpus of text. Version/source control of text with git facilitating distributed collaboration on texts, history and tracking of changes, and attribution of authorship of individual contributions to a text. Reproducible computational environments with containerization technologies such as docker and Continuous integration/deployment pipelines to automate testing, checking and building and deployment/publication of software products. The economics and game theory of proposed solutions as well as gaining the trust and confidence of the academic community are given central importance here and the technical considerations are a means to this end. What I describe here aspires to be a unified vision for bringing together a number of different reforms, you may disagree with my analysis and/or proposed solutions for a particular problem or even that something that I point to as a problem is one. Whilst I think the separate elements of this fit together into a cohesive whole I would urge the reader to consider the merits of the specific proposals separately as I think some are individually valuable. I welcome critique and feedback on all of this content. This document is published as a ‘git-book’ using bookdown (Xie 2020) if you would like to discuss it’s content please do so in the discussion section associated with git repository on github if you have a specific problem, fact check or correction please open an issue, if you would like to suggest an edit, and are willing to specify the exact changes you’d like to see made please open a pull request. References "],["units-of-publication.html", "Section 2 Units of publication 2.1 The new hierarchy of publication types 2.2 The minimum unit of publication", " Section 2 Units of publication 2.1 The new hierarchy of publication types The traditional hierarchy of scientific publications looks something like this: Textbooks Reviews Primary papers Running from the narrowest to the broadest scope and most detail to the highest level of synthesis and abstraction, in theory. This is an oversimplification and we have been adding to the ontology of publication types somewhat in recent years, more on this later. Some additions such as pre-registrations have caught on more in some field that others. Whilst some innovations are reasonably domain specific others could benefit from being more widely adopted. In addition to the mainline scientific communication hierarchy there is of course the communication of science to a broader audience. Ranging from cross disciplinary communication to communicating with the public, policy makers, and in education. These essential functions of the scientific community with immense civic import that are undervalued in the current model. Integrating many of the new forms of publication that have arisen I would propose a new more granular hierarchy: Science communication (multimedia, curricula, policy briefs etc.) “git-books” - much like a textbook but under continuous revision Topic Reviews, Curated Best practice resources for lab protocols and computational pipelines as well as, Benchmarking resources Theory, synthesis and prediction (without a requirement for experimental detail but ideally with resolution criteria) Experimental results Data, Methods, Protocols, Pipelines, &amp; Software publications pre-registrations / experimental plans/designs We have made some progress towards this model already, everything that I list here already exists in some form after all. We have however not really thought or talked about them much when put together as a whole - at least not in my experience. Bringing this more granular view of the unit scientific publication to the fore I think has a number of advantages. Consider implications of new forms of publication for existing ones 2.2 The minimum unit of publication Let us step back for a moment a consider what is the minimum unit of publication? According to the proponents of ‘nanopublications’ this minimal unit of publication is a simple statement consisting of a subject, object predicate triple that is generated by some author and asserted by some party (Groth, Gibson, and Velterop 2010). e.g. the journal of the widely understood asserts that the statement “Malaria is transmitted by mosquitoes” was authored by John Smith. This minimal unit of publication structured in this fashion has a number of very useful properties for automating the creation of a web of semantic meaning in the scientific literature. Such structure has considerable potential to increase the ease and effectiveness of literature mining and may permit semi-automated means of assessing the weight of evidence supporting a particular statement and identifying gaps and weaknesses it the current state of understanding of particular topics. semantic web rdp graph structure etc. https://en.wikipedia.org/wiki/Semantic_Web current nano-publication infrastructure need and of work before it will be a smooth easy to use experience for most would be authors and I suspect also some major performance optimizations for handling large datasets. However, it would be somewhat cumbersome and impractical to write only in nano-publications, whilst they may be convenient for a machine legibility they are less convenient for human legibility and I envisage them catching on once at least semi-automated tools can convert a more human unit of publication into a collection of nano-publications. Modern publications particularly in the life sciences (my home territory) are too long, too complex, too narrative driven and, in partial contradiction with being too long are often too compressed. I’ll be expanding on these points, but for now take my word for it that this is not merely me complaining about trying to read 50 page cell papers and still have the time to do anything else. As the ability to do experimental work has increased in the life sciences due to technological advances that have rendered some projects that would have taken decades 20-30 years ago achievable in months the expectations for publications have changed. Reading papers from the 1980’s and 90’s one is struck by their relative shortness and simplicity compared to many modern papers. Indeed, empirically scientific manuscripts are getting harder to read (Plavén-Sigray et al. 2017). Check the data on length, has it actually increased? I would argue that is this mostly not due to an intrinsic increase in the complexity of the systems we are able to study and the methods that we are using to study them. These have increased and some of the added difficulty may be accounted for by the increasing number and narrowness of specialties but I think more blame is due to failure to adapt institutional structures around ever more and narrower specialties as well as publication practices. Annecdotally it is my impression that the number of experiments and the technical complexity of those experiments per paper has been on the rise (I would welcome empirical evidence from literature mining on this subject to confirm or disconfirm my conjecture here). It is marvelous that we can get more done and that we have sophisticated new tools with which to work, however this presents a number of problems when publishing. Firstly for the peer review process: For an increasing number of papers the scope of what they investigate exceeds the reasonable ability of the typical number of reviewers to adequately assess the material. Domain expertise in the system being studied, statistical expertise, methodological expertise in the methods employed are all needed for a rigorous assessment of most papers. The larger and more complex the unit of publication the greater the opportunity for diffusion of responsibility among the reviewers who may be free to assume that one of the others will give adequate scrutiny to aspects outside their direct expertise, this may not be the case. As papers cover an increasing number, and more complex experiments the probability of error in some part of the work increases, as the more things are in a unit of publication the more chances there are for at least one of them to be in error. I suspect that error increases at a rate greater than that which you would expect from simple conjunction, due in part to the diffusion of responsibility for review earlier hypothesized earlier. I would contend that the increasing complexity of papers contributes to the replication problems commonplace in many field in part because of the challenges of adequately reviewing them. I propose experimental tests of the diffusion of responsibility in reviewers and effectiveness of reduced publication size/scope at reducing it. For example one could send a manuscript with a statistical error to two types of sets of reviewers. one pool of reviewers none of whom have stats expertise and count how often to they a. catch the error, b. flag their lack of expertise and ask for review by a stats expert c. take no action. The second set of reviewers should contains a stats expert to establish a base rate for missing the error. Try this for a conventional publication and ‘micropublication’ containing essentially only the experiment with the error. That addresses criticisms of too long and complex, now for the related problem of being what I think of as too compressed. The text of papers in nature for instance is almost always to short to adaquately convey the necessary detail for the amount of work that has been done to justify a place in such a prestigious publication. It is a primary publication so must be precise and near exhaustive in its description of the work done Excessive narrative seeking can contribute to the file-draw effect, if you can’t make a good story of it you might not publish it. The expectation of turning work into a ‘full paper’ can also have bad incentives, for some project you go out on a lim and it does not work in way that left you write a full paper but data generated and other aspects of the work are This greater specialization of publication types reduces citation dilution, increases reviewability by narrowing the scope of expertise necessary to evaluate smaller publications. It also pushes the making of narratives away from the primary research where it can contribute to the file draw problem and pushes it to it’s own ‘sesense making’ layer. This also reduces time to publication it can take years to get things published under the current paradigm, this should tell us publications are too big and too complex and need to be broken into smaller parts to shorten the feedback loop of the review process. This stack also enables a new form of professional scientific specialization not purely in the dimension of subject specialty but at level of analysis, you could specialize in sharpening up the theoretical models or experimental design in a broarder array of diciplins publishing mostly at the theory level or generate a lot of high quality data. High level synthesis, curation and communication of important scientific discovery in the field as gitbook maintainers to keep the ‘textbooks’ current on an ongoing basis without the constraint of editions whilst remaining citable because of version control. Increases accessibility to publishing to students and non-domain experts making smaller contributions that are still in the public interest to have published. It may be argued this this approach increases the amount of ‘noise’ there are already so many publications that it is impossible to keep up in many fields, this is part of why new specialties are needed in the synthesis of experimental information into models. It is only possible for individuals to keep up will all the primary publications in a few very narrow specialties… Another element that this model contributes in greater codification of understanding that often goes un codified both at the level of specific methodological detail and at the level of the mental models that researchers are using ot reason about their research. currently lacking a self regulatory mechanism to govern the balance between publications of the different types so we don’t end up with a bunch of unanalysed data / too much theorizing? Don’t get me wrong there are a lot of good papers out there but increasingly this seems to be in spite of and not because of the publication medium. smaller papers = fewer co-authors, thus reduced collective action problem of choice to publish in alternate publications if you don’t know your co authors that well you are more likely to assume that they will want to publish in conventional high impact journals and not be too keen on you deciding to publish in a less fashionable venue so we default to the conservative option of a conventional option rather than risk getting into an extended email argument about the benefits of open science with collaborators who we want to keep happy. We are stuck in a bad nash equilibrium because of our assumptions about our co-authors and they may not be as valid as we conservatively expect. better documentation of informal replications performed as precursosrs to other work References "],["incentives.html", "Section 3 Incentives 3.1 The Review bounty", " Section 3 Incentives Aligning the incentives of authors, publishers &amp; reviewers to favor good science What do we need from a journal and how can we incentivise that and only that reviewer / author matching prioritisation / curation type setting, proofing plagarism, hosting In the current publishing ecosystem incentives are often actively bad and when they are not actively bad they are often pretty neutral with respect to the key outcome of good quality scientific research. pit of sucess The Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks. To the extent that we make it easy to get into trouble we fail. goodhart’s law issue with metrics/impact - resolve through alignment 3.1 The Review bounty A new funding model for academic publishing I propose a new system for funding academic publications which I call review bounties. If you want to publish a manuscript you don’t pay a publication fee you put up a bounty for it’s review. If you wish to be a publisher you don’t put up a pay-wall and charge an access subscription you take a cut of the review bounty. As an author you take what you want to publish to a pre-print and review server you state how much you are willing to pay to have it reviewed and published and you make ranked choice list of venues you would be interested in publishing your manuscript. Publications then have the option to take you up on overseeing the review and possible publication of your manuscript. They contact reviewers who agree to review the manuscript in exchange for an agreed cut of the review bounty. Reviews are open, not anonymous, as are all payment amounts. This openness is essential to prevent grift as if a journal permits you to simply pay some friends for an easy review their reputation as a venue for rigorous review must suffer. There are a number of interesting variations you can make to this basic model. consider the ‘bug bounty’ in which a portion of the review bounty is held in reserve and anyone who finds an error in the work that is of material relevance to the conclusions of the work can claim the bounty. This can be decided by the senior author, a majority of the authors or a consensus of the publisher and original reviewers. The bug bounty accrues back to the authors, publisher and reviewers over time, rewarding them for making non-obvious errors and incentivizing bug hunters to find errors quickly and thus get larger rewards. In “An Incentive Solution to the Peer Review Problem” (Hauser and Fehr 2007) Marc Hauser &amp; Ernst Fehr propose that reviewers who fail to deliver their reviews on time be penalized by having any papers they subsequently seek to have published at the journal for which their review was late be held up twice as long as they made other authors wait. This proposal may incentivize timely review but punishing tardy reviewers in this way has serious negative externalities, it delays the publication of potentially important work and will disincentivize anyone who has ever been late with a review from publishing in that journal again. This is not likely to be convenient for the journal or the reviewer. Overall this does not seem like good game theory outside of a very narrow scope, when designing incentive systems we must keep in mind that the end goal is better science we can’t optimize solely for punctual reviewers. A solution to this problem in the review bounty system is that a reviewer forfeits the bounty if a review is not provided by the deadline. Then another reviewer can claim it, or the original reviewer can reclaim part of it but only if they now provide their review up-front prior to another reviewer claiming the bounty. The longer an author is kept waiting beyond the deadline the more of their bounty they recoup with the cost comming partly from the publisher to incentivise speedy organisation of the review. Anonymity and secrecy in peer review is in my opinion thoroughly over-rated, it can conceal abuses of power like holding up someone else’s paper so you or your mate can publish first as much or more than it provides scope for junior people to critique the work of senior people. Openness increases the reputational risk to senior people of petty behaviour like denying someone a job or grant application because they canned your paper. Make a habit of that sort of thing and people will now notice and hold you to account. If you can’t both respectfully take and dish out an intellectually rigorous critique what are you doing in this line of work? 3.1.1 poor incentives for quality publishing formatting general friction around formatting file types and styling catering to pagination and the dead tree figure panels lack of useful animation/interactivity declining readability Scientific manuscripts are getting harder to read (???) Citation Dilution (the bigger a paper the less specific you are being when you reference it) ~ credit dilution / attribution? scope beyond reviewer competence particularly in biology the are massive papers published whith 10 years ago would have taken 10 years to complete can now be a sinlge complec paper narative seeking / lack of pre-registration/distinction between the predicted and the post hoc commitment to metaresearch to improve research publications A public benefit corporation like model or equivolent corporate structure to help prevent a return to the current rachet review bounties as a key component of the attracting quality reviewer time away from established publishers costly signaling theory in publishing - jumping through arbitary hoops to signal prestige? make it harder to fake signals, gaming metrics/acceptance with low cost noise that sounds good but lacks substance, stronger openness norms and smaller units give this bad quality signal less space to hide physics archive - decoupling of publishing from archiv, pre-prints don’t solve the problem need a new hybrid solution ? croud funded news and views bounties? pol.is and quadratic voting for review/synthesis subject selection such a platform has the potential to create a transparent competative market for publication services especially if aided by regulatory or other institutional interventions that penalise the predatory business practices of current industry players steven novella - minimum publishable unit vs minimal unit of publication, emphasis publishable several crappy experiments and a story, if experiment have to stand on their own they will need to be of higher quality not obfucated by being strung together in a narrative. see also sciencematters.io - ‘single observation publishing’ see the science breaker (universite de geneve) lay summaries / news and views commision (bounties from anyone?) The authors of the The tragedy of the reviewer commons (https://doi.org/10.1111/j.1461-0248.2008.01276.x) (Hochberg et al. 2009) identify as an issue: “attempting to publish the smallest acceptable unit” this appears to conflict with proposals for ‘micropublications’ or other smaller units of publication but I would argue that the underlying concerns that lead to calling this an issue arise from conceiving as a ‘conventional’ paper as the minimum viable unit of unit publication. reducing the size of the unit of publication from a full paper to a single experiment, dataset, protocol description, experimental design, theoretical model etc. This makes it easier to do your best work because the scope of the task is smaller, it also make it easier to review effectively If we conceive of the minimum unit of scientific publication as what has become a conventional paper with several often complex experiments and an overarching narrative then an underdeveloped paper can indeed be burdensome to review. We have a compounding problem of increasing technical complexity of papers due to in part rising expectations in the amount of work going into a paper to make it worthy of a ‘top tier’ journal and increasing volume of articles. The authors of ‘The tragedy of the reviewer commons’ (Hochberg et al. 2009) pre-review by colleagues - do this on platform and don’t waste time circulating a bunch of copies by email and reconciling the resulting comments, have and invited reviewers feature that lets you invite input on your work prior to submitting it for formal review reviewing a modern paper is a daunting task - uncertainty about the expertise of the other reviewers what is the subset of things in the paper which you are expected to the expert reviewer on are the areas outside your scope adaquately covered by other reviewers? - I full expect people often tacitly assume the editors have handled this adequately as it theoretically their responsibility and just review what they feel comfortable reviewing. medium/form mismatch - way to much for the short format paper References "],["medium-matters.html", "Section 4 Medium Matters 4.1 We need to talk about Multi-panel figures 4.2 Interactivity, Animation, &amp; More 4.3 Objections to dropping pdfs", " Section 4 Medium Matters The tragic tyranny of typeset pdfs in a less paginated era. The pdf reigns supreme as the medium in which academic papers must be published and oppresses us all with an iron fist. OK, maybe that’s a bit much but I’m dead serious about the fact that the anachronistic favoring of form in a dead medium over function in a current one is causing us issues larger than they might at first appear. The dead tree is no longer medium on which most of us read academic papers, the screen now rules in this domain. Yet we still format papers to be nicely typeset and printed, we even still organize figures together into panels so that the publishers need not spend as much money on color printed pages. What’s wrong this this you could reasonably say, what’s the problem with having papers nicely typeset for printing? Nothing per se it would be fine if we were still reading the paper versions but we are not we are reading them electronically and typeset for print is not the best way to present information for easy comprehension on a screen. Case-in-point try and read a pdf of a paper on a phone with a screen no larger than about 5\", torturous right? Whilst the mismatch is less extreme on a nice big desktop monitor there are still non-optimal aspects of reading something meant for print on a monitor just a little less extreme than the phone case. The problem I think is a combination two things, information non-locality and poorer spacial information mapping on screens vs paper. What do I mean by information non-locality? When the information needed to understand something is spread widely through the manuscript and you have to go to many different places to piece together all the bits of information you need to understand a thing. For a concrete example let us take the what in my opinion are the worst offenders in this space, Multi-panel figures. 4.1 We need to talk about Multi-panel figures (Apologies if my personal loathing of multi-panel figure comes through overmuch in the style and tone here, I tried to exercise some restraint) What information do you usually need to interpret a panel in a figure? The figure itself The figure legend The place in the text which references the figure more often than not at least one disambiguation of a novel acronym Each of these 4 pieces of information can be located on different pages in a modern manuscript. Yes the figure and it’s legend are sometimes of different pages (I’m looking at you Cell WTF?). To drive this point home you can be looking at a situation like this: Figure 6i was discussed on page 4, printed on page 5 and the legend for which is, for some ungodly reason, printed on page 6! Oh and also the title for the figure is not on the figure but in the legend and helpfully features (YATA) yet another terrible acronym that was defined once in the abstract on page 1. I don’t know about you but a mere mortal such as myself needs some working memory spare to think about the underlying concepts that a manuscript is trying to communicate to me, and I do not appreciate having to cache as many a 4 look-up tables in my head just to be able to effectively read a figure, let alone reason further about it! I content that this problem is worse when looking at electronic representations of formats meant to be printed, at least in part because it is easier to form a mental map of the relationships between these different parts of the manuscript when it is printed. When you have a physical copy the parts have relationships to one another in 3D space that take less effort to construct a mental model of. Also it’s just easier to arrange the pages so can actually look at the different parts and the same time. e-reader like formats also suffer from this issue as they don’t have fixed pagination so the relationships between objects are harder to model still as they are not even in fixed relationship to one another in a virtual 2D space. I hypothesize that breaking up figure panels placing information, such as titles and abbreviations unnecessarily relegated to figure legends in the figure itself and placing figures in line in the text as they occur would increase reading speed and comprehension, especially in electronic media. I term this information-non locality minimization. Specific hypotheses I’d like to see tested: Time taken to read a paper with multi-panel figures will be longer than time taken when information-non locality is minimised. Reading comprehension tests will be worse for papers with multi-panel figures than when information-non locality is minimised. Hypotheses 1 &amp; 2 will be true of both electronic and physical media but the effect size will be larger for electronic media. Bound copies of papers with multi-panel figures will provide less of an advantage over information-non locality minimised copies than loose leaf / unbound prints. Effect size of the advantage information-non locality minimized versions have over multi-panel figure versions will be larger in people with dyslexia. (I’m dyslexic and it’s been thought of as an issue related to working memory) If anyone wants to do this research get in touch I might even be willing to contribute to funding it. To sum up Multi-panel figures are an anacronistic concession to typesetting color prints to a compact format for printing and they have no place in modern electronic publishing. They ruin the flow of reading a paper. They represents an unreasonable assault on the working memory resources of a reader who is trying to understand what is likely a complex topic and create undue cognitive load for readers who need their full mental faculties about them to graple with complex scientific ideas. Additionally I think that they encourage bad graphical practices. Panels are labeled not with meaningfully interpretable titles as both our schooling and basic common sense about presenting information dictates they should be but with alphabet soup. These are bad practices for which we take school children to task for in science classes but seem to have collectively forgotten in the peer reviewed literature. Multi-panel figures should be used ONLY when it is actually useful for the understanding of the content for visuals to be placed together. We don’t need to optimize for printing in electronic formats we can have as many full size color figures as we need to optimally convey our point. This would be an improvement even if we stick to a pdf as the primary end point paradigm which I will be making a case against later. (Rant over 😉 ) We will now be asking, what can you get when you ditch the static pdf as the primary end point? 4.2 Interactivity, Animation, &amp; More There is some skepticism over the value of interactivity and especially animation in actually being effective at communicating usefully interpetable scientific information. Just as there a many ways to produce terrible uninformative graphs there are many ways for interactivity and animation to be ineffectually employed but there are a number of areas in which they excel at effective communication in ways not easily captured in static plots. To make the point about interactivity I will largely let this little widget from canvasXpress make my point for me. Imagine ever plot in a scientific paper worked like the widget below. No seriously make it full screen and play around with it a bit, right click in the plot and take a look at the options. Look at the code I wrote to get this level of functionality, I’ve left it visible above the plot - yes that’s it. library(canvasXpress) # TODO: make a better example plot canvasXpress::canvasXpress(mtcars) I’m serious really play with it before you move on, have you tired double clicking, have you looked in configure? This sort of capability is particularly valuable for looking at publications of exploratory analyses where the authors publishing the work Let’s Consider some other uses of interactivity. You know those ‘representative images’ people put in their papers when they do a bulk analysis of images what if the representative image was a widget that picked one at random from the appropriate group in the dataset deposited at the Image Data Archive and changed it periodically? A norm like this might have a positive effect on the representativeness of those images don’t you think? 4.2.1 The utility of animation for representing uncertainty 4.3 Objections to dropping pdfs “We can’t get rid of pdfs we need a version of the paper that we can archive!” I hear you cry. Yes, we do archival formats are important but the archival format does not have to be prettily typeset to minimize the number of color prints there are archival formats as good or better than pdfs In Section 5.2 Computational Irreproducibility &amp; In Section 6 Key Technologies I will be making the case that a form of plain text document should be the definitive reference version, key end point and primary authoring format. The separation of semantic content from formatting information (css / html) why plain text? parseability - interoperability with many downstream text processing tools small, version control, temporal stability of format, open "],["computational-pitfalls.html", "Section 5 Computational Pitfalls 5.1 Computational Transparency (or lack thereof) 5.2 Computational Irreproducibility", " Section 5 Computational Pitfalls 5.1 Computational Transparency (or lack thereof) Very few papers published today don’t contain at least one computer generated graph and a statistical test. Computing is ubiquitous in research and growing ever more so. The chances are therefore, that somewhere in any given paper you publish there is a number generated from your data by a computer. Now there are plenty of sources of error that can be introduced between your data and the number that ends up in your paper. These can be simple errors like typos and bugs in software, conceptual errors like using the wrong tool and a variety of other failure modes. We can with some of the technologies I outline in the next section eliminate many of the sources of simple error and ensure that any conceptual mistakes are at least properly documented so that we can tell if we for them look carefully. My goal for the new publishing medium is that the published artifact should provide a complete description of every function applied to the input data to produce the outputs. That is to say if a number or plot appears in your manuscript then the published artifact should show how that was derived from your data. In the most minimal example consider I want to do some arithmetic, why should I ask you to trust me that I did the calculation right when I can write the calculation out and have the computer evaluate it? If I want to tell you the value of x is 2652 but the fact that I computed it by doing \\(x=\\frac{7957}{3}\\) does not need to be in the manuscript at this point I can put the computation specification in the source document that generates the manuscript. In the case for the number above it is a piece of code that looks like this: ` r round(7957/3) ` That way you can check my work and I can’t accidentally transpose a number copying the output into my manuscript. I can still introduce bad data (garbage in garbage out) and I can still specify and incorrect computation but now you can see what the computation is and if I misspecified it. This generalizes from the simple case to complete complex computational pipelines and tools to make this quite achievable in practice now exist as we will see later. This might seem like quite a bit of trouble to go to, why am I making this rather exacting auditability a priority for the future of scientific publications? We’ll see in the next section. 5.2 Computational Irreproducibility It is hard to reproduce, not to replicate but merely to exactly redo analyses published in many papers. Complex bioinformatic analyses can take months to reproduce and often simply cannot be done without contacting their authors for additional information (Garijo et al. 2013). Much progress on this issue has been made, (by some), since the publication of that study however best practices are under-defined, inconsistently applied, not taught well/widely and not well incentivised. This latter point is very important there is little incentive when publishing to ensure computational reproducibility. Few reviewers will flag this as an issue. Also we are quite accustomed to seeing phrases like “analysis was performed with custom in-house python scripts”. This should worry us, I’ll be expanding a little on why below. I’ve been guilty of this sort of vagueness before I learned some of the tools to make it practical to avoid this. These scripts may or may not be made available with the manuscript and if they are it may be impossible to get them to run if details of the computational environment like software versions are not also provided. Without details of the computational environment there is also no guarantee that even if it runs it will produce exactly the same output. This can be at multiple levels from package versions you are using to the OS you are running. find the reference for the bioinf tool that produced different results on macOS and linux. This would be bad enough but software has bugs, lots of bugs. Highly paid teams of professional programmers with years of experience and automated testing frameworks pump out software used by millions of people that is full of bugs. There are plenty of bugs in software written for highly regulated industries with exacting safety standards like the military, aviation, and medical devices. Some of these bugs have killed people, others have cost companies millions (cite humble pi) The key point is that highly motivate professionals find it impossible to eliminate all bugs even when lives depend on it. Most code written by academics for the purposes of doing data analyses is not subject to much scrutiny it rarely has to survive a ‘production environment’ also much of it, no offense, is written by semi-self taught amateurs. How buggy is what we write likely to be when we don’t have nearly as rigorous a process or nearly as much exposure to real world stresses to reveal errors as professionally written code? This applies to us all even if you don’t write what is conventionally thought of as code. Ever used and excel function? congratulations you written some code and it might be buggy. Now there is good reason for us academics to not expend vast efforts on the sort of rigorous software engineering someone like NASA would do, and I’m not saying we should. We need to be able to prototype rapidly, to make horrendously non-performant proofs of principle, to hack things together, make things so specialized that they only need to run for our specific problem and it doesn’t matter terribly if they are slow or not user friendly. We do however need to be able to check our work, and that of our colleagues. We need an audit trail of what we did and how so that we can track down and understand sources of error. The same way that we keep detailed lab notebooks that can help us spot mistakes and improve protocols at the bench, we need analogous processes for our computational work. let’s return to excel, have you ever sorted by a column in excel and got the dialog box asking if you want to expand your selection to all columns or just re-order the one you selected? If you picked the latter option congratulations you just effectively randomized the order of one column with respect to the others. It is quite easy to do this and it can be quite hard to tell if you have if you don’t notice immediately. beyond the ephemeral undo history there is also no record of this operation stored in excel so you can’t easily do some forensics to figure out where it all went wrong. This is a reason to prefer writing code to working with a GUI without a detailed and persistent history of the operations you have performed in a form that you can share with others and use to repeat your analysis exactly. When working with software are working with high level abstractions that crystallize an immense body of understanding often opaque to us as individuals behind the click of a button or a function call. It is thus essential to the verifiability (Hinsen 2018) of our work that we can document how we used these tools so that others with appropriate expertise can assess the technical correctness of the tool and the validity of our use of it for the question at hand. As we become more specialized and the paths of inference from observations to conclusions become longer and more convoluted than can fit in the scope of our individual knowledge. As a result we must strengthen the approaches we take to insuring correctness of the chain of inferences at the weak points where is passes from one domain of specialized knowledge to another. This lack of reproducibility need not be the case, we have the technical means to overcome this issue with tools primarily originating in the modern software development and deployment industry. In the next section I will be covering how we can make use of some of these technologies to overcome some of the pitfalls outlined here and even to make the process of writing a publishing research a more streamlined, less painful and more rigorous process than the way be do it now. References "],["key-technologies.html", "Section 6 Key Technologies 6.1 Literate Programming 6.2 Collaboration and version control with git 6.3 Containers and build pipelines 6.4 Why not Blockchain (yet)?", " Section 6 Key Technologies The key technologies to implement all of the ideas proposed here already exist. What is needed is a software solution which ties a number of them together in a particular way that fits the use case of writing, reviewing and publishing academic manuscripts. 6.1 Literate Programming The case for writing in plain text Why would I write in a plain text file when I could use something like word or google docs? I used to be a word aficionado I new all the fancy features, and when real-time collaborative editing came along in google docs I was on-board and longing for reference management integrations. Now I write almost exclusively in plain text files, I can still do all the same stuff I did in word and google docs but also much more. I do this even when I’m not coding I wrote my thesis like this, I’m doing it now. I can work in an editor that is a bit more like word or google docs in appearance but because I often write code mixed in with my prose I generally don’t. I’m using a simple set of rules for formatting my text document called markdown. Here is ~98% of all the markdown syntax you are likely to need to know: # Heading 1 ## Heading 2 ## Heading 3 {#h3} a reference to a section \\@ref(h3) - bullet point - another point 1. *italics!* 2. __Bold__ [hyperlink](https://en.wikipedia.org/wiki/Hyperlink) Images: [!Alt text](path/to/image) an inline reference to @key123 or a parenthasized one [@key123] inline `code` ``` # A block of code print(&quot;Hello, World!&quot;) ``` If using mathjax or a simialar tool: inline latex math: $\\frac{x}{y}$ Equations: $$ a=\\frac{x}{y^2} $$ Pretty simple right? If you can learn to use some of the slightly more advanced features of MS Word like styles, sections, track changes or automated reference management you can learn markdown pretty easily. Also there are now some quite nice visual editors to give you more of the WYSIWYG editing experience if you need a leg-up with the syntax, there is one built right into the tool I’m authoring this document in right now RStudio. It even has a nice feature that will let me insert references from my Zotero library or by DOI search and add them to the bibliography for this bookdown project. But why do I do this? for one reason as I write this it is being rendered every time this file is saved to a window I have open on my second monitor so I can see what you will be seeing. That is if you are reading this on the website because I’m also generating an epub and pdf document from my source text. If I felt so inclined I could add a few lines of configuration and also generate a word version, or use I could use rticles to format it in the style of a paper from my favorite journal. I could also try for various forms of slideshow including powerpoint but this is a bit verbose for slides. The point is you get a lot of different output options from the same starting point. This is because writing this way makes use of a key insight that is now central to good web design, the separation of the semantic content from how it is formatted. If like me you are old enough to have been taught to write webpages in school in the early 2000’s or to have had a myspace page you may remember setting formatting rules for text in html tags. Setting the color of some text in an html tag is now anathema to the web designer, formatting is done with css not in the html! Why? Because you can completely change the look, layout, navigation or the way you display the same content if you structure things this way, that’s why I’m able to render this document in three different formats at a keystroke. Its semantic structure is governed by the very simple rules outline above, from that this text can be parsed into a data structure that permits it to be formatted and output in a myriad different ways. This also allows for much improved accessibility as a sane order for screen readers inherent in the structure of the source document and what ever size and typeface you’d like this represented in is up to you. As well as easy text mining and searching that does not require wrestling this text back out of a pdf into a nice structure for computers to work with. Also as I alluded to above I often write a mixture of prose and code this is called Literate Programming where the documentation to my code and my description of what it does is interwoven with the code. It is from this concept that I get the title of this piece “Literate science” my proposition is that we should move to a paradigm of authoring academic manuscripts that resembles literate programming. To be clear I’m not suggesting that everyone learns to code nor that our manuscripts should be detailed documentation of code. What I am suggesting is that the source document that generates your manuscript should generate every number and figure in that manuscript from your input data. You shouldn’t need to learn to code to do this. You may, however, need to use tools which generate code which is inserted into the source document for your manuscript, instead of using tools where there is no means of exactly reproducing the steps you took in the analysis you performed with a gui tool (see section 5.2). Remember that interactive plot I told you to play with back in section 4.2 on in 1 6.2 Collaboration and version control with git git is a tool used by programmers to keep track of and navigate through the history of changes they have made to code, also to collaborate on code. Originally developed by Linus Torvalds to facilitate the distributed development of the Linux kernel this tool can be applied as well to any body of text as is can to computer source code. This document I’m writing now is in a git repository. It is keeping track of the differences between what is in the files I’m editing now and the last time I ‘commited’ my previous set of changes. With it I can navigate through all my past changes, and revert to previous versions should I choose to. Each commit is associated with a unique hash so if you needed to cite a very specific version of this document you could identify the commit hash associated with the exact version where the change you are interested in was made. This is a very useful feature of git for the purposes of academic publishing, the ability to cite a repository and a specific version of that repository permits us to handle updates and retractions much more smoothly. If you go to a I can collaborate on this text asynchronously with others using git. If they ‘fork’ off their own ‘branch’ of the project and make some changes we can ‘merge’ them back together looking at all the places our versions differ and choosing which to keep. It also keeps track of who authored which parts of the text, a potentially useful feature for the attribution of authorship credit but with the clear limitation of capturing only who actually wrote the text. One could also do google docs style real-time collaborative editing of your source document and then commit the resultant changes with git using a tool such as hackmd.io. git has a fairly rich and complex feature set but these are the key ones for our purposes and they can all be accomplished with fairly easy to use gui interfaces such as those implemented at github and git lab. However a git interface tailored to the needs of document authors and not explicitly software development would be a useful addition to the ecosystem and something I would want to see a a part of the platform I’m proposing. 6.3 Containers and build pipelines git and gitlab for versioning in collaborative plaintext document editing markdown as a simple to use plaintext markup lanugage Rmarkdown/kintr/pandoc with Rstudio as document editor/renderer literate programming gitlab ci-cd / docker for reproducible computational environments and automated checking automatically see if your edit to a manuscript if that change breaks a formatting rule automated QC checking and helping, readability analysis, plagarism, stats checks etc. 6.4 Why not Blockchain (yet)? You might want to skip this if your not a crytpo person, it’s mostly to explain to people on the crypto train while I’m not on board yet for this project All this stuff about putting up a bounty, being able to claim that bounty, having amounts of money accrue back to certain parties, doing compute, covering the cost of compute, being open and public sounds like the sort of thing that would gel really interestingly with blockchain with smart contracts and distributed compute? - I hear you ask. Yes, I answer, but the tooling is not there and the ecosystem is not yet mature enough to support what I am proposing here. most of tech stack to support my proposals exists in stuff that was not built to talk to blockchains. I expect to see a version of what I propose built on blockchain technology in the future but I anticipate a timescale on the order of decades. In addition It would be a barrier to the wider adoption of my proposed approach until such time as crypto has gained wider acceptance and understanding. Like it or not justified or not crypto has in the minds of many some negative connotations. I’m already asking for quite a big jump from people crypto is like a bridge to far. There are many potential advantages: Cyptographically backed identity management for authors, very robust provenance of ideas in a public record, preventing the raising of barriers to knowledge or censorship by governmental of corporate entities through its decentralized nature, etc. A strong crytographic base might useful for the implementation of certain security tools such as limiting data asses to approved individuals with verified credentials and access rights. e.g. allowing access to data only if an individual has an api key signed by by several parties such as their personal identification token to demonstrate it is them and by the tokens of an ethics review board. It may also be possible to do things like run analyses on data to which the researcher is never granted access directly and the code to run the analysis has to be signed by a technical review board who have checked the code is not able to exfiltrate the data it is given access to for analysis. The ability to transact and have ‘smart contracts’*, to govern the review bounty process does provide a potential upside of this tech especially when it comes to dealing with potential regulatory hurdles associated with this. Any situation where you are holding money which you may at some point be obliged to give back to another party tends to attract a lot of red tape. Cardano is I think the most likely candidate in the current crypto market to be the basis for such a system given it’s technical characteristics and its governance structure’s fondness for peer-review. (Declaration of interest: I have some modest investments in cardano, etherium, bitcoin and monero; cardano having the largest share) However in short almost all of the aims I outline here and even many of those stated by people trying to deploy blockchain tech in this space, such as DEIP &amp; VitaDAO, can be achieved without the use of blockchain technology. Things like author contribution metrics tied to git commits, (imperfect as these would be as a representation of relative contribution), and reputation metrics associated with an author’s ID, even crytographically verifiable IDs, can be accomplished with a PGP web of trust a yubikey and existing git tools like github/gitlab. Even much of the decentralized model is achievable through a federated architecture, don’t get me wrong I’m bullish on crypto I just don’t think the layer 2/3 tech that I think this approach to publishing would need to be blockchain-native is ready for prime-time just yet. Happy to be convinced otherwise, but I’m going to want to see some running code. *This is a terrible name BTW, automated contracts would be a lot more accurate, basic conditional logic != smart. NB Markdown’s competitors that largely lost the format wars reStructuredText (rST) and AsciiDoc would arguably be a better choice for the text-based lingua franka format of academic publication going forward but the tooling and ecosystem around these is just not quite as mature.↩︎ "],["the-platform.html", "Section 7 The platform", " Section 7 The platform To achieve this I propose the following: - New commercial publications and publication platforms should be incorporated using ‘public benefit’ or ‘social purpose’ like corporate structure and not conventional ‘C’ corporations. (I’m working here with my limited knowledge of US corporate structures, not all jurisdictions even, as I understand it, in the US have these types of corporate entities yet). It is my understanding that under US law publicly traded companies are under an fiduciary duty to maximise shareholder profit lest they expose themselves to liability for lost earnings by their shareholders, with private equity carrying the expectation of eventually going public making back the investment. This produces a misalignment between corporate entities and their customers with the company incentivised to attempt to exploit their existing customer base for short term stock price gains instead of entering into a long term positive sum interaction with their customers and serving the intended purpose of the corporation to provide the services of an academic publisher. A ‘B corp’ on the other hand is given license to prioritize its purpose over maximizing its bottom line. - We base new publications on open source software stacks. (by which I actually mean free/libre software to appease (a fellow Richard)[fsf.org] and fellow pedant). I propose the development of open publication platforms by ‘B corps’ (see above). The business model I envision for them is similar to that employed by many open source software based businesses such as RedHat and SUSE. Individual journals would pay for support for their own instances, consulting, some bespoke development for their needs that will be committed to the public code base and SaaS style hosting for groups who want and instance without the technical overhead. The SaaS service must always have feature parity with the community version. Instances would be part of a federated network with the ability to use accounts on one instance to interact with another instance akin to matrix, mastadon or peertube and likely employing OCRID for universal academic identification. It is essential that such a publishing platform be built on a free software license this permits the publisher to pre-commit to a business model in which they cannot treat their customers unethically without risking incurring significant costs. With a publishing platform based on such a license a company that develops it must continue to add value or it faces a credible threat that the codebase could be forked by a competitor or a community effort. As such as platform would offer it services over a network it should ideally make use of a license which protects against loss of free software protections by offering the software as a service such as the AGPL. licensing under the AGPL and taking community contributions without a contributor license agreement (CLA), by which contributor grant the company the copyright to their individual contributions, serves as an extremely effective pre-commitment strategy to keeping the software free as the software cannot then be re-licensed without the consent of all contributors. formative peer review https://publicphilosophyjournal.org/overview (lessons from plants) federated pre-print pool in addition to AI pulicatoin matching - similarity metric for your text to profile for a journal Publication could submit an offer to review a preprint (offer dicounts for choosing them?) universial review transfer between possible publications on the platform An open review process where a submitted manuscript is a pre-print and a formative peer review process may or may not result in it becoming a formally peer review process and which enables collaborations when additional work is needed to to to a publication quality peice of work federated article submission platform where you submit not to a single journal to multiple ones - I’m still thinking about the incentive structure engineering on this last point possibilities include journals bidding on articles with reduced fees, more reviewer time, authors providing ranked wight of first refusal list and voting systems to help rank interesting candidates. it remains the case that automated reference management does not play well with collaborative document editing - versioning and track changes hell Each journal can specify things such as how long each section is allowed to be with hard and soft limits, the same applies to the number of figures and references "],["adoption.html", "Section 8 Adoption", " Section 8 Adoption Out-competing the incumbants on community trust In order for new companies starting out in the publication space to suceed in addressing the issues that have arisen from the extractive behaviors of the current publishing racket/oligopoly they must out compete the conventional publishing houses in a number of dimensions. They must do so not merely in ease of use, novel features and scholarly merit, but also in the trust of the academic community that they are not going to become the very thing they sought to destroy and dash our hopes of lasting and effective reform of academic publishing. The academic community must be able to have confidence that the new journals will not be captured by the industry incumbents and lead to the same old problems recurring. Look how Mendeley stagnated after the Elsevier aquisition and began introducing anti-features like encrypting local databases reducing user data probability. Prioritise features that will make it appealing to use for authors/reviewers - editing UI, collaboration tools, reference management - directly apply reviewer editor changes to fix e.g. typos as PRs - needs guidance for how best to use this for all, build in onboarding - journal template linting - this is a unique feature - A new company starting out in this space federated network based on an open source technology the federated model permits ‘B corp’ small practical benefits pit of sucsess - raise the bar at the low end to make firm foundations for building peaks but not expecting all to be peak feats ‘go to market strategy’ starts with applying this concept where the Unique selling point of reproducible computation is valuable and the level of technical knowledge is high so the rough edges on UI and effective onboarding can be handled over time. This starting market is bioinformatics and other disciplines cognisent of the need for computational reproducibility and with some members already comfortable with things like R notebooks. (let me know if you work in field which fits this description) start with a publishing house for such specially journals as an on paper a subsidiary or separate company of the main platform development company and expand into other markets. Academic society journals are in trouble at the moment, they often worked on the closed model and are finding it hard to transition to the open pay to publish model. Their overheads for running a journal are quite high this platform would aim to majorly reduce those administrative overheads, making it easier for smaller groups like academic societies to publish a journal This is another niche to persure once the initial ease of use kinks are worked out in the more technical communities. "],["references.html", "References", " References "]]
